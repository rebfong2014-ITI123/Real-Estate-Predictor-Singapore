# %% [markdown]
# # ITI123 Milestone Project: Real Estate Price Prediction (DNN)
#
# **Student Name:** FONG Siang Yi
# **Student ID:** 8440104B
#
# ## 1. Setup and Initialization
# Importing necessary libraries for data manipulation, preprocessing, and deep learning.

# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Concatenate, Embedding, Flatten
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

print(f"TensorFlow Version: {tf.__version__}")


# Load dataset (Assumes data is in 'property_data.csv')
# If you are running this in Colab/Binder, upload the CSV first.
try:
    df = pd.read_csv('property_data.csv')
except FileNotFoundError:
    print("Dataset not found. Creating a synthetic sample for demonstration purposes...")
    # Creating synthetic data to match the user's schema for demonstration
    data = {
        'Purchaser Address Indicator': np.random.choice(['HDB', 'Private', 'Landed'], 1000),
        'Postal District': np.random.randint(1, 28, 1000),
…        'Log Unit Price ($ psm)': np.random.uniform(3, 5, 1000), # Note: Potential leakage if not careful
        'Log Unit Price ($ psf)': np.random.uniform(2, 4, 1000), # Note: Potential leakage if not careful
        'Log Transacted Price': np.random.uniform(5, 7, 1000)    # TARGET
    }
    df = pd.DataFrame(data)


# Define Feature Groups
TARGET = 'Log Transacted Price'

# NOTE: 'Log Unit Price' columns are excluded from input (X) because they are directly derived
# from the Transaction Price (Target) and Area. Using them would cause Data Leakage.
numerical_cols = ['Distance to MRT', 'Distance to Primary School', 'Age of Unit', 'GDP', 'SORA']
categorical_ohe_cols = ['Purchaser Address Indicator', 'Planning Region', 'Partition']
categorical_embed_cols = ['Postal Sector', 'Planning Area'] # High cardinality for Embeddings

# 1. Handle Missing Values
# Fill numerical with median, categorical with 'Unknown'
for col in numerical_cols:
    df[col] = df[col].fillna(df[col].median())

for col in categorical_ohe_cols + categorical_embed_cols:
    df[col] = df[col].fillna('Unknown')

# 2. Split X and y
X = df.drop(columns=[TARGET, 'Log Unit Price ($ psm)', 'Log Unit Price ($ psf)', 'Postal Code', 'Postal District'])
# Dropped Postal Code/District in favor of Sector/Region to reduce redundancy/noise
y = df[TARGET]

# 3. Preprocessing Pipelines

# Standard Scaling for Numerical Data
scaler = StandardScaler()
X_num = scaler.fit_transform(X[numerical_cols])

# One-Hot Encoding for Low Cardinality Categoricals
ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
X_cat_ohe = ohe.fit_transform(X[categorical_ohe_cols])

# Label Encoding for Embedding Layers
# We need to map strings to integers (0 to N-1) for the Embedding layer
feature_inputs = {}
encoders = {}

# Prepare dictionary to hold processed columns
X_processed = {
    'numerical': X_num,
    'ohe': X_cat_ohe
}

for col in categorical_embed_cols:
    le = LabelEncoder()
    # Fit transform and cast to int32 for TensorFlow
    X_processed[col] = le.fit_transform(X[col].astype(str)).astype('int32')
    encoders[col] = le
    print(f"Feature '{col}' - Vocabulary Size: {len(le.classes_)}")

# First split: Train (70%) and Temp (30%)
# First split: Train (80%) and Temp (20%)
indices = np.arange(len(df))
train_idx, temp_idx = train_test_split(indices, test_size=0.3, random_state=42)
#train_idx, temp_idx = train_test_split(indices, test_size=0.3, random_state=42)
val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)

def get_split(data_dict, idx):
    return {key: val[idx] for key, val in data_dict.items()}

X_train = get_split(X_processed, train_idx)
X_val = get_split(X_processed, val_idx)
X_test = get_split(X_processed, test_idx)

y_train = y.iloc[train_idx].values
y_val = y.iloc[val_idx].values
y_test = y.iloc[test_idx].values

print(f"Training samples: {len(y_train)}")
print(f"Validation samples: {len(y_val)}")
print(f"Test samples: {len(y_test)}")

#added on 16 Feb 2026
# Display the first few observations of our train data
train_df = df.iloc[train_idx].copy() # Create a DataFrame for the training data
print("Training Data Head:")
train_df.head()


def build_model():
    inputs = []
    embeddings = []

    # 1. Numerical + OHE Input
    input_dim_dense = X_train['numerical'].shape[1] + X_train['ohe'].shape[1]
    input_dense = Input(shape=(input_dim_dense,), name="numerical_ohe_input")
    inputs.append(input_dense)

    # 2. Embedding Inputs
    # Flatten() is needed to convert (batch, 1, embed_dim) -> (batch, embed_dim)

    # Postal Sector Embedding
    input_sector = Input(shape=(1,), name="Postal_Sector_Input")
    inputs.append(input_sector)
    vocab_sector = len(encoders['Postal Sector'].classes_)
    embed_sector = Embedding(input_dim=vocab_sector + 1, output_dim=34, name="Embed_Sector")(input_sector)
   # embed_sector = Embedding(input_dim=vocab_sector + 1, output_dim=10, name="Embed_Sector")(input_sector)
    embed_sector = Flatten()(embed_sector)
    embeddings.append(embed_sector)

    # Planning Area Embedding
    input_area = Input(shape=(1,), name="Planning_Area_Input")
    inputs.append(input_area)
    vocab_area = len(encoders['Planning Area'].classes_)
    embed_area = Embedding(input_dim=vocab_area + 1, output_dim=19, name="Embed_Area")(input_area)
    #embed_area = Embedding(input_dim=vocab_area + 1, output_dim=5, name="Embed_Area")(input_area)
    embed_area = Flatten()(embed_area)
    embeddings.append(embed_area)

    # 3. Concatenate
    x = Concatenate()([input_dense] + embeddings)

    # 4. Deep Dense Layers
    x = Dense(256, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)

    x = Dense(128, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)

    x = Dense(64, activation='relu')(x)

    # 5. Output Layer
    output = Dense(1, activation='linear', name="price_prediction")(x)

    model = Model(inputs=inputs, outputs=output)
    return model

model = build_model()
model.summary()


model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])

# Prepare input lists for Keras (must match order of inputs defined in build_model)
def format_inputs(X_dict):
    # numerical_ohe_input, Postal_Sector_Input, Planning_Area_Input
    dense_input = np.hstack([X_dict['numerical'], X_dict['ohe']])
    return [dense_input, X_dict['Postal Sector'], X_dict['Planning Area']]

train_inputs = format_inputs(X_train)
val_inputs = format_inputs(X_val)
test_inputs = format_inputs(X_test)

history = model.fit(
    train_inputs, y_train,
    validation_data=(val_inputs, y_val),
    epochs=600,
    #epochs=100,
    batch_size=64,
    callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],
    verbose=1
)


# %% [markdown]
# ## 7. Evaluation and Discussion
#
# Evaluating the model on the unseen Test set using RMSE, MAE, and R².

# %%
# Predictions
y_pred = model.predict(test_inputs).flatten()

# Metrics
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("="*30)
print("       TEST SET RESULTS       ")
print("="*30)
print(f"RMSE : {rmse:.4f}")
print(f"MAE  : {mae:.4f}")
print(f"R²   : {r2:.4f}")
print("="*30)

# Visualization: Predicted vs Actual
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_test, y=y_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Actual Log Transacted Price')
plt.ylabel('Predicted Log Transacted Price')
plt.title('Actual vs Predicted Property Values (DNN)')
plt.show()

# Visualize Training History
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Model Training Loss (MSE)')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

# Visualize Training History
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Model Training Loss (MSE)')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()


